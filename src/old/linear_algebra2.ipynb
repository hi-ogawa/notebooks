{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook linear_algebra2.ipynb to python\n",
      "[NbConvertApp] Writing 9482 bytes to linear_algebra2.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert linear_algebra2.ipynb --to python --no-prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# - Conjugate gradient descent\n",
    "# - Cholskey decomposition\n",
    "# - LR decomp with pivot selection (Gauss elim.)\n",
    "# - Bidiagonalization of positive definite (with numba)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import unittest\n",
    "import numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_0 (__main__.__0)\n",
      "Standard ... ok\n",
      "test_1 (__main__.__0)\n",
      "Preconditioned ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: #iters: 444, error: 4.2854521361943334e-09\n",
      ":: #iters: 757, error: 2.0565085520205685e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.393s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "def conjugate_gradient_descent(A, b, num_steps=2**10, debug=False):\n",
    "    M, N = A.shape\n",
    "    K, = b.shape\n",
    "    assert M == N and M == K\n",
    "    loop = itertools.count() if num_steps is None else range(num_steps)\n",
    "\n",
    "    x = np.zeros_like(b)\n",
    "    r = A @ x - b\n",
    "    dot_r = np.dot(r, r)\n",
    "    if np.allclose(dot_r, 0): return x\n",
    "    p = r\n",
    "    \n",
    "    for i in loop:\n",
    "        # line search minimum\n",
    "        alpha = - np.dot(p, r) / np.dot(p, A @ p)\n",
    "        x += alpha * p\n",
    "        \n",
    "        # gradient\n",
    "        r_ = A @ x - b\n",
    "        \n",
    "        # conjugate gradient\n",
    "        dot_r_ = np.dot(r_, r_)\n",
    "        beta = dot_r_ / dot_r\n",
    "        p = r_ + beta * p\n",
    "\n",
    "        # update\n",
    "        r = r_\n",
    "        dot_r = dot_r_\n",
    "        \n",
    "        if debug and (i + 1) % 2**10 == 0:\n",
    "            print(f\":: #iters: {i}, error: {dot_r}\")\n",
    "        if np.allclose(dot_r, 0):\n",
    "            break\n",
    "\n",
    "    if debug: print(f\":: #iters: {i}, error: {dot_r}\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def conjugate_gradient_descent_preconditioned(A, b, C, num_steps=2**10, debug=False):\n",
    "    \"\"\"\n",
    "    Equivalent problem\n",
    "    - (M A MH) y = M b\n",
    "    - x = MH y\n",
    "    - r = M A MH y - M b\n",
    "    - r = M s \n",
    "    - q = MH p\n",
    "    - C = MH M\n",
    "    \n",
    "    Follow equivalent problem's `r` and `q` in comments within code.\n",
    "    It's amazing that we don't use M and MH for computation and C saficies.\n",
    "    \"\"\"\n",
    "    M, N = A.shape\n",
    "    K, = b.shape\n",
    "    assert M == N and M == K\n",
    "    loop = itertools.count() if num_steps is None else range(num_steps)\n",
    "\n",
    "    x = np.zeros_like(b)\n",
    "    s = A @ x - b\n",
    "    # r = M @ s\n",
    "    # p = r\n",
    "    # q = MH @ p = MH @ M @ s = C @ s\n",
    "    q = C @ s\n",
    "    \n",
    "    # <r, r> = <M s, M s> = <s, MH M s> = <s, C s>\n",
    "    cs = C @ s\n",
    "    dot_r = np.dot(s, cs)\n",
    "    if np.allclose(dot_r, 0): return x\n",
    "    \n",
    "    for i in loop:\n",
    "        # line-search minimum\n",
    "        alpha = - np.dot(q, s) / np.dot(q, A @ q)\n",
    "        x += alpha * q\n",
    "\n",
    "        # gradient\n",
    "        s_ = A @ x - b\n",
    "        cs_ = C @ s_\n",
    "        \n",
    "        # conjugate gradient\n",
    "        # - p' = r' + (<r', r'> / <r, r>) p = M s' + (<s', C s'> / <s, C s>) p\n",
    "        # - q' = MH p' = MH M s' + (<s', C s'> / <s, C s>) MH p\n",
    "        #              = C s' + (<s', C s'> / <s, C s>) q\n",
    "        dot_r_ = np.dot(s_, cs_)\n",
    "        beta = dot_r_ / dot_r\n",
    "        q = cs_ + beta * q\n",
    "\n",
    "        # update\n",
    "        s = s_\n",
    "        cs = cs_\n",
    "        dot_r = dot_r_\n",
    "        \n",
    "        if debug and (i + 1) % 2**10 == 0:\n",
    "            print(f\":: #iters: {i}, error: {dot_r}\")\n",
    "        if np.allclose(dot_r, 0):\n",
    "            break        \n",
    "\n",
    "    if debug: print(f\":: #iters: {i}, error: {dot_r}\")\n",
    "    return x\n",
    "\n",
    "\n",
    "class __0(unittest.TestCase):\n",
    "    \"\"\"Conjugate Gradient Descent\"\"\"\n",
    "    \n",
    "    def test_0(self):\n",
    "        \"\"\"Standard\"\"\"\n",
    "        \n",
    "        np.random.seed(0)\n",
    "        N = 2**8 # 2**11 worked\n",
    "        A = np.random.randn(N, N)\n",
    "        A = A.T @ A # this should be positive definite with probability 1\n",
    "        b = np.random.randn(N)\n",
    "        \n",
    "        x = conjugate_gradient_descent(A, b, debug=True, num_steps=None)\n",
    "        r = A @ x - b\n",
    "        e = np.dot(r, r)\n",
    "        assert np.allclose(e, 0)\n",
    "        \n",
    "    def test_1(self):\n",
    "        \"\"\"Preconditioned\"\"\"\n",
    "        \n",
    "        np.random.seed(0)\n",
    "        N = 2**7 # 2**11 worked\n",
    "        A = np.random.randn(N, N)\n",
    "        A = A.T @ A # this should be positive definite with probability 1\n",
    "        b = np.random.randn(N)\n",
    "        \n",
    "        # Use random pos. def. as preconditioner\n",
    "        C = np.random.randn(N, N)\n",
    "        C = C.T @ C \n",
    "        \n",
    "        x = conjugate_gradient_descent_preconditioned(A, b, C, debug=True, num_steps=None)\n",
    "        r = A @ x - b\n",
    "        e = np.dot(r, r)\n",
    "        assert np.allclose(e, 0)        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv='XXX -v -k __0.test_'.split(), exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_0 (__main__.__1)\n",
      "Real matrix ... ok\n",
      "test_1 (__main__.__1)\n",
      "Complex matrix ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.020s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "def lower_tri_mask(A):\n",
    "    M, N = A.shape\n",
    "    return np.array([[i >= j for j in range(N)] for i in range(M)])\n",
    "\n",
    "def cholesky(A):\n",
    "    \"\"\"\n",
    "    A = L LH\n",
    "    where\n",
    "    - L: left triangle\n",
    "    \n",
    "    Input:\n",
    "    - A: positive definite\n",
    "\n",
    "    Output:\n",
    "    - L\n",
    "    \"\"\"\n",
    "    M, N = A.shape; assert M == N\n",
    "    pass\n",
    "\n",
    "def cholesky_nonstandard(A):\n",
    "    \"\"\"\n",
    "    It seems this is \"non standard\" version of existence proof.\n",
    "    Run induction from right bottom part of sub matrix, which I feel more natural.\n",
    "    \"\"\"\n",
    "    M, N = A.shape; assert M == N\n",
    "    for i in range(M):\n",
    "        a = A[i, i]\n",
    "        b = A[i+1:, i]\n",
    "        A[i+1:, i+1:] -= np.outer(b, np.conj(b)) / a # only lower triangle should be updated\n",
    "        A[i:, i] /= np.sqrt(a)\n",
    "    A *= lower_tri_mask(A) # keep only lower triangle\n",
    "\n",
    "class __1(unittest.TestCase):\n",
    "    \"\"\"Cholesky decomposition test\"\"\"\n",
    "    \n",
    "    def test_0(self):\n",
    "        \"\"\"Real matrix\"\"\"\n",
    "        np.random.seed(0)\n",
    "        N = 2**7\n",
    "        A = np.random.randn(N, N)\n",
    "        A = np.conj(A.T) @ A # this should be positive definite with probability 1\n",
    "        \n",
    "        L = A.copy()\n",
    "        cholesky_nonstandard(L)\n",
    "        assert np.allclose(L @ np.conj(L.T), A)\n",
    "        \n",
    "    def test_1(self):\n",
    "        \"\"\"Complex matrix\"\"\"\n",
    "        np.random.seed(0)\n",
    "        N = 4\n",
    "        Ar = np.random.randn(N, N)\n",
    "        Ai = np.random.randn(N, N) * 1j\n",
    "        A = Ar + Ai\n",
    "        A = np.conj(A.T) @ A # this should be positive definite with probability 1\n",
    "\n",
    "        L = A.copy()\n",
    "        cholesky_nonstandard(L)\n",
    "        assert np.allclose(L @ np.conj(L.T), A)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv='XXX -v -k __1.test_'.split(), exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.          0.          0.          0.        ]\n",
      " [ 0.52030783  1.          0.          0.          0.        ]\n",
      " [-0.70468315 -0.32957863  1.          0.          0.        ]\n",
      " [-0.45566661  0.24200367  0.61701845  1.          0.        ]\n",
      " [ 0.29936943  0.58903618 -0.25137662 -0.05545173  1.        ]]\n",
      "\n",
      "[[-1.70627019  1.9507754  -0.50965218 -0.4380743  -1.25279536]\n",
      " [ 0.         -2.99580019 -0.08273613  0.38428246  1.88212992]\n",
      " [-0.         -0.         -0.68871411 -1.23060526 -1.68253191]\n",
      " [-0.          0.          0.         -0.42877401  0.39871637]\n",
      " [ 0.          0.         -0.         -0.         -1.06791644]]\n",
      "\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# LR decomposition (via Gaussian elim.)\n",
    "#\n",
    "import numpy as np\n",
    "\n",
    "def pivot_policy__maximum_abs(A):\n",
    "    v = np.abs(A[:, 0])\n",
    "    i = np.argmax(v)\n",
    "    if v[i] < np.finfo(v.dtype).eps:\n",
    "        return None\n",
    "    else:\n",
    "        return i\n",
    "\n",
    "#\n",
    "# Proof sketch\n",
    "#   L0 P0 A0 = A1\n",
    "#   L1 P1 A1 = A2\n",
    "#   => \n",
    "#   A2 = L1 P1 A1\n",
    "#      = L1 P1 L0 P0 A0\n",
    "#      = L1 (P1 L0 P1) (P1 P0) A0\n",
    "#            ~~~~~~~~ = L0'\n",
    "#\n",
    "def lr_decomp(A):\n",
    "    M, N = A.shape; assert M == N\n",
    "    pivots = np.zeros((M,), dtype=np.int)\n",
    "    pivots[-1] = M - 1\n",
    "\n",
    "    for i in range(M - 1):\n",
    "        pivot = pivot_policy__maximum_abs(A[i:, i:])\n",
    "        \n",
    "        # None means the column is already \"eliminated\"\n",
    "        if pivot is None:\n",
    "            pivots[i] = i\n",
    "            continue\n",
    "        \n",
    "        # Save pivot\n",
    "        j = i + pivot\n",
    "        pivots[i] = j\n",
    "        \n",
    "        # Pivoting\n",
    "        if i != j:\n",
    "            A[i], A[j] = A[j], A[i].copy()\n",
    "        \n",
    "        # Elimination\n",
    "        a = A[i, i]\n",
    "        b = A[i+1:, i]\n",
    "        l = b / a\n",
    "        c = A[i, i+1:]\n",
    "        A[i+1:, i+1:] -= np.outer(l, c)\n",
    "\n",
    "        # Save L's column in the unused space\n",
    "        A[i+1:, i] = l\n",
    "\n",
    "    return pivots\n",
    "\n",
    "def separate_lr(LR):\n",
    "    M, N = LR.shape; assert M == N\n",
    "    idx_diff = np.arange(M).reshape(-1, 1) - np.arange(M)\n",
    "    left_mask = idx_diff > 0\n",
    "    diag_mask = idx_diff == 0\n",
    "    L = LR * left_mask + np.eye(M)\n",
    "    R = LR * ~left_mask\n",
    "    return L, R    \n",
    "\n",
    "def pivots_to_permutation(pivots):\n",
    "    N, = pivots.shape\n",
    "    P = np.eye(N)\n",
    "    for i in range(N):\n",
    "        j = pivots[i]\n",
    "        if i != j:\n",
    "            P[i], P[j] = P[j], P[i].copy()\n",
    "    return P\n",
    "\n",
    "def test():\n",
    "    A = np.random.randn(5, 5)\n",
    "    LR = A.copy()\n",
    "    pivots = lr_decomp(LR) # inplace\n",
    "    L, R = separate_lr(LR)\n",
    "    P = pivots_to_permutation(pivots)\n",
    "    print(L, R, P, sep='\\n\\n')\n",
    "    assert np.allclose(P @ A, L @ R)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f64dedb892ab>:33: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (array(float64, 1d, A), array(float64, 2d, A))\n",
      "  Q[i+1:, :] -= 2 * np.outer(u, u @ Q[i+1:, :])\n",
      "<ipython-input-8-f64dedb892ab>:34: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (array(float64, 1d, A), array(float64, 2d, A))\n",
      "  A[i+1:, i:] -= 2 * np.outer(u, u @ A[i+1:, i:])\n",
      "<ipython-input-8-f64dedb892ab>:35: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (array(float64, 2d, A), array(float64, 1d, A))\n",
      "  A[i:, i+1:] -= 2 * np.outer(A[i:, i+1:] @ u, u)\n",
      "/home/hiogawa/.local/share/virtualenvs/python--UUats5H/lib/python3.7/site-packages/numba/typing/npydecl.py:977: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (array(float64, 1d, A), array(float64, 2d, A))\n",
      "  warnings.warn(NumbaPerformanceWarning(msg))\n",
      "/home/hiogawa/.local/share/virtualenvs/python--UUats5H/lib/python3.7/site-packages/numba/typing/npydecl.py:977: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (array(float64, 1d, A), array(float64, 2d, A))\n",
      "  warnings.warn(NumbaPerformanceWarning(msg))\n",
      "/home/hiogawa/.local/share/virtualenvs/python--UUats5H/lib/python3.7/site-packages/numba/typing/npydecl.py:977: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (array(float64, 2d, A), array(float64, 1d, A))\n",
      "  warnings.warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "\n",
    "def band_mask(n, b):\n",
    "    k = b - 1\n",
    "    r = np.arange(n)\n",
    "    mask = np.abs(r[:, np.newaxis] - r) <= k\n",
    "    return mask\n",
    "\n",
    "@numba.njit('f8[:](f8[:],)')\n",
    "def householder_vector(v):\n",
    "    # assert v.shape[0] >= 1\n",
    "    v_norm = np.linalg.norm(v)\n",
    "    if v_norm == 0:\n",
    "        return v\n",
    "    a = v[0]\n",
    "    a_abs = np.abs(a)\n",
    "    u = v.copy()\n",
    "    if a_abs == 0:\n",
    "        u[0] = v_norm\n",
    "    else:\n",
    "        u[0] = (a / a_abs) * (a_abs + v_norm)\n",
    "    u_norm = np.sqrt(2 * v_norm * (v_norm + a_abs))\n",
    "    return u / u_norm\n",
    "\n",
    "@numba.njit('Tuple((f8[:, :], f8[:, :]))(f8[:, :])')\n",
    "def bidiag(A):\n",
    "    # assert A.shape[0] == A.shape[1]\n",
    "    # assert A is hermitian\n",
    "    m = A.shape[0]\n",
    "    Q = np.eye(m)\n",
    "    for i in range(m - 2):\n",
    "        u = householder_vector(A[i+1:, i])\n",
    "        Q[i+1:, :] -= 2 * np.outer(u, u @ Q[i+1:, :])\n",
    "        A[i+1:, i:] -= 2 * np.outer(u, u @ A[i+1:, i:])\n",
    "        A[i:, i+1:] -= 2 * np.outer(A[i:, i+1:] @ u, u)\n",
    "    return Q, A\n",
    "\n",
    "def _test_bidiag(A, verbose=False):\n",
    "    x = np.linspace(0, 1, 2**2)\n",
    "\n",
    "    # \"squared exp\" covariance function\n",
    "    cov = np.exp(- (x[:, np.newaxis] - x) ** 2)\n",
    "    Q, B = bidiag(A.copy())\n",
    "\n",
    "    # B: bidiagonal\n",
    "    assert np.allclose(B[np.tril_indices(B.shape[0], -2)], 0)\n",
    "    assert np.allclose(B[np.triu_indices(B.shape[0], +2)], 0)\n",
    "    \n",
    "    # Q: unitary\n",
    "    if verbose:\n",
    "        plt.colorbar(plt.matshow(Q @ Q.T))\n",
    "        plt.colorbar(plt.matshow(B))\n",
    "        plt.colorbar(plt.matshow(Q))\n",
    "        \n",
    "    assert np.allclose(Q @ Q.T, np.eye(Q.shape[0]))\n",
    "\n",
    "    # Unitary equivalence\n",
    "    assert np.allclose(Q @ A @ Q.T, B)\n",
    "    \n",
    "def test_bidiag_ex1():\n",
    "    # \"squared exp\" covariance function\n",
    "    x = np.linspace(0, 1, 2**5)\n",
    "    cov = np.exp(- 1 * (x[:, np.newaxis] - x) ** 2)\n",
    "    _test_bidiag(cov, verbose=False)\n",
    "    \n",
    "def test_bidiag_ex2():\n",
    "    A = np.random.randn(2**7, 2**7)\n",
    "    _test_bidiag(A @ A.T)\n",
    "\n",
    "def test_bidiag_ex3():\n",
    "    A = np.random.randn(8, 4)\n",
    "    _test_bidiag(A @ A.T, verbose=False)\n",
    "\n",
    "test_bidiag_ex1()\n",
    "test_bidiag_ex2()\n",
    "test_bidiag_ex3()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
