{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Minimal yaml-like language supporting only seven line patterns\n",
    "#\n",
    "\n",
    "# [Supported cases]\n",
    "#   case p0. xyz\n",
    "#   case p1. xyz:\n",
    "#   case p2. xyz: abc\n",
    "#   case p3. -\n",
    "#   case p4. - xyz\n",
    "#   case p5. - xyz:\n",
    "#   case p6. - xyz: abc\n",
    "\n",
    "def parse_line(line):\n",
    "    # @return\n",
    "    #   pattern: int \\in -1, 0, .., 6\n",
    "    #   val1: str\n",
    "    #   val2: str\n",
    "    q1 = line[-1:] == ':'\n",
    "    q2 = len(line.split(': ')) == 2\n",
    "    q3 = line[:1] == '-'\n",
    "    q4 = line[:2] == '- '\n",
    "    \n",
    "    p0 = not any([q1, q2, q3])\n",
    "    p1 = q1 and not any([q3])\n",
    "    p2 = q2 and not any([q3, q4])\n",
    "    p3 = line == '-'\n",
    "    p4 = q4 and not any([q1, q2])\n",
    "    p5 = q4 and q1\n",
    "    p6 = q4 and q2\n",
    "\n",
    "    typ = -1\n",
    "    for i, p in enumerate([p0, p1, p2, p3, p4, p5, p6]):\n",
    "        if p:\n",
    "            typ = i\n",
    "            break\n",
    "    \n",
    "    val1, val2 = None, None\n",
    "    if typ == 0: val1 = line\n",
    "    if typ == 1: val1 = line[:-1]\n",
    "    if typ == 2: val1, val2 = line.split(': ')\n",
    "    if typ == 3: pass\n",
    "    if typ == 4: val1 = line[2:]\n",
    "    if typ == 5: val1 = line[2:-1]\n",
    "    if typ == 6: val1, val2 = line[2:].split(': ')\n",
    "\n",
    "    return typ, val1, val2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "State = types.SimpleNamespace\n",
    "\n",
    "def separate_indent(line): # str -> (int, str)\n",
    "    new_line = line.lstrip()\n",
    "    indent = len(line) - len(new_line)\n",
    "    return indent, new_line\n",
    "\n",
    "\n",
    "def separate_lines(lines, indent): # [str], int -> ([str], [str])\n",
    "    inner_lines = []\n",
    "    n = 0\n",
    "    for line in lines:\n",
    "        new_indent, new_line = separate_indent(line)\n",
    "        if new_indent < indent:\n",
    "            break\n",
    "        inner_lines.append(line)\n",
    "        n += 1\n",
    "    return inner_lines, lines[n:]\n",
    "\n",
    "\n",
    "def parse_rec(lines, state, debug=False):\n",
    "    # @param\n",
    "    #   lines: [str]\n",
    "    #   state (inout)\n",
    "    #     tmp: Yaml\n",
    "    #     indent: int\n",
    "    # @return\n",
    "    #   result: Yaml\n",
    "    #   lines: [str]\n",
    "    if len(lines) == 0:\n",
    "        return state.tmp, []\n",
    "\n",
    "    line, *lines = lines\n",
    "    curr_indent, curr_line = separate_indent(line)\n",
    "    if debug:\n",
    "        print(f\"[debug:{state.indent}] tmp = {state.tmp}\")\n",
    "        print(f\"[debug:{state.indent}] line = {line}\")\n",
    "\n",
    "    assert state.indent == curr_indent, f\"[parse_rec] Not supported indent : \\\"{line}\\\"\"\n",
    "\n",
    "    #\n",
    "    # Parse line\n",
    "    #\n",
    "    p, val1, val2 = parse_line(curr_line)\n",
    "    if debug:\n",
    "        print(f\"[debug:{state.indent}] p = {p}\")\n",
    "    message = f\"[parse_rec] Not supported line pattern ({p}) : \\\"{line}\\\"\"\n",
    "    if p == 0:\n",
    "        assert type(state.tmp) == type(None), message\n",
    "    if p in [1, 2]:\n",
    "        assert type(state.tmp) in [type(None), dict], message\n",
    "        if type(state.tmp) == type(None):\n",
    "            state.tmp = {}\n",
    "    if p in [3, 4, 5, 6]:\n",
    "        assert type(state.tmp) in [type(None), list], message\n",
    "        if type(state.tmp) == type(None):\n",
    "            state.tmp = []\n",
    "\n",
    "    #\n",
    "    # Update temporary result (state.tmp) or return based on 7 patterns\n",
    "    #\n",
    "    if p == 0:\n",
    "        return val1, lines\n",
    "\n",
    "    if p == 1:\n",
    "        inner_state = State(tmp=None, indent=state.indent + 2)\n",
    "        inner_lines, lines = separate_lines(lines, inner_state.indent)\n",
    "        inner_result, _ = parse_rec(inner_lines, inner_state, debug=debug)\n",
    "        state.tmp[val1] = inner_result\n",
    "\n",
    "    if p == 2:\n",
    "        state.tmp[val1] = val2\n",
    "        \n",
    "    if p == 3:\n",
    "        inner_state = State(tmp=None, indent=state.indent + 2)\n",
    "        inner_lines, lines = separate_lines(lines, inner_state.indent)\n",
    "        inner_result, _ = parse_rec(inner_lines, inner_state, debug=debug)\n",
    "        state.tmp.append(inner_result)\n",
    "\n",
    "    if p == 4:\n",
    "        state.tmp.append(val1)\n",
    "\n",
    "    if p == 5:\n",
    "        inner4_state = State(tmp=None, indent=state.indent + 4)\n",
    "        inner4_lines, lines = separate_lines(lines, inner4_state.indent)\n",
    "        inner4_result, _ = parse_rec(inner4_lines, inner4_state, debug=debug)\n",
    "\n",
    "        inner2_state = State(tmp={val1 : inner4_result}, indent=state.indent + 2)\n",
    "        inner2_lines, lines = separate_lines(lines, inner2_state.indent)\n",
    "        inner2_result, _ = parse_rec(inner2_lines, inner2_state, debug=debug)\n",
    "        \n",
    "        state.tmp.append(inner2_result)\n",
    "\n",
    "    if p == 6:\n",
    "        inner2_state = State(tmp={val1 : val2}, indent=state.indent + 2)\n",
    "        inner2_lines, lines = separate_lines(lines, inner2_state.indent)\n",
    "        inner2_result, _ = parse_rec(inner2_lines, inner2_state, debug=debug)        \n",
    "        \n",
    "        state.tmp.append(inner2_result)\n",
    "\n",
    "    #\n",
    "    # Parse rest of the lines\n",
    "    #\n",
    "    return parse_rec(lines, state, debug=debug)\n",
    "\n",
    "\n",
    "def preprocess_lines(lines):\n",
    "    def gen():\n",
    "        for line in lines:\n",
    "            # Ignore line comment\n",
    "            if '#' in line:\n",
    "                line = line[:line.index('#')]\n",
    "                \n",
    "            # Ignore trailing whilespaces \n",
    "            line = line.rstrip()\n",
    "            \n",
    "            # Ignore empty line\n",
    "            if len(line) > 0:\n",
    "                yield line\n",
    "\n",
    "    return list(gen())\n",
    "\n",
    "\n",
    "def parse(text, debug=False):\n",
    "    lines = text.splitlines()\n",
    "    lines = preprocess_lines(lines)\n",
    "    state = State(tmp=None, indent=0)\n",
    "    result, lines = parse_rec(lines, state, debug=debug)\n",
    "    assert lines == [], \"Not all text is not consumed\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Example input\n",
    "#\n",
    "\n",
    "ex00 = \"\"\"\\\n",
    "camera:\n",
    "  type: MyCamera\n",
    "  params:\n",
    "    camera_loc: (1, 1, 1)\n",
    "    lookat_loc: (0, 0, 0)\n",
    "    up_vec: (0, 1, 0)\n",
    "    \n",
    "scene:\n",
    "  type: MyScene\n",
    "  params:\n",
    "    file: data/bunny/reconstruction/bun_zipper_res2.ply\n",
    "\n",
    "integrator:\n",
    "  type: NormalIntegrator\n",
    "  params:\n",
    "\"\"\"\n",
    "\n",
    "ex01 = \"\"\"\\\n",
    "key1:\n",
    "  key1-1:\n",
    "    - 1\n",
    "    - 2: 3\n",
    "    -\n",
    "      4: 5\n",
    "    - 6:\n",
    "        7: 8\n",
    "  key1-2:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: ex00\n",
      "{\n",
      "  \"camera\": {\n",
      "    \"type\": \"MyCamera\",\n",
      "    \"params\": {\n",
      "      \"camera_loc\": \"(1, 1, 1)\",\n",
      "      \"lookat_loc\": \"(0, 0, 0)\",\n",
      "      \"up_vec\": \"(0, 1, 0)\"\n",
      "    }\n",
      "  },\n",
      "  \"scene\": {\n",
      "    \"type\": \"MyScene\",\n",
      "    \"params\": {\n",
      "      \"file\": \"data/bunny/reconstruction/bun_zipper_res2.ply\"\n",
      "    }\n",
      "  },\n",
      "  \"integrator\": {\n",
      "    \"type\": \"NormalIntegrator\",\n",
      "    \"params\": null\n",
      "  }\n",
      "}\n",
      "\n",
      ":: ex01\n",
      "{\n",
      "  \"key1\": {\n",
      "    \"key1-1\": [\n",
      "      \"1\",\n",
      "      {\n",
      "        \"2\": \"3\"\n",
      "      },\n",
      "      {\n",
      "        \"4\": \"5\"\n",
      "      },\n",
      "      {\n",
      "        \"6\": {\n",
      "          \"7\": \"8\"\n",
      "        }\n",
      "      }\n",
      "    ],\n",
      "    \"key1-2\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(\":: ex00\")\n",
    "print(json.dumps(parse(ex00, debug=False), indent=2))\n",
    "print()\n",
    "\n",
    "print(\":: ex01\")\n",
    "print(json.dumps(parse(ex01, debug=False), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ab\"c', 7)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_string_literal(line):\n",
    "    # @param line : str\n",
    "    # @return\n",
    "    #   result : str\n",
    "    #   num    : int (number of character consumed from `line`)\n",
    "    assert line[0] == '\\\"'\n",
    "    i = 1\n",
    "    s = ''  # result string\n",
    "    while True:\n",
    "        c = line[i:i+1]\n",
    "        i += 1\n",
    "        \n",
    "        # error\n",
    "        ls = dict([\n",
    "            ['',   'invalid EOI'],\n",
    "            ['\\n', 'invalid EOL'],\n",
    "        ])\n",
    "        assert (c not in ls), ls[c]\n",
    "\n",
    "        # finish\n",
    "        if c == '\"':\n",
    "            break\n",
    "            \n",
    "        # escape character\n",
    "        if c == '\\\\':\n",
    "            cc = line[i:i+1]\n",
    "            i += 1\n",
    "            assert cc, 'invalid backslash'\n",
    "            ls = dict([\n",
    "                ['n',  '\\n'],\n",
    "                ['\\\"', '\\\"'],\n",
    "                ['\\\"', '\\\"'],\n",
    "                ['\\\\', '\\\\'],\n",
    "            ])\n",
    "            assert (cc in ls), f\"invalid escape character \\\\{cc}\"\n",
    "            s += ls[cc]\n",
    "            continue\n",
    "            \n",
    "        # normal character\n",
    "        s += c\n",
    "\n",
    "    return s, i\n",
    "\n",
    "# test\n",
    "read_string_literal(\"\"\"\\\n",
    "\"ab\\\\\"c\" jsdfj\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Tokens\n",
    "#   Minus, Colon, Key, String, Indent, Dedent, Newline, EOI\n",
    "#\n",
    "# Grammer\n",
    "#   Input = Newline* Expr Newline* EOI\n",
    "#   Expr = SimpleExpr | CompoundExpr\n",
    "#   SimpleExpr   = String Newline\n",
    "#   CompoundExpr = Dict | List\n",
    "#   Suite = SimpleExpr | (Newline Indent Expr Dedent)\n",
    "#   Dict = DictItem+\n",
    "#   List = ListItem+\n",
    "#   DictItem = Key Suite\n",
    "#   ListItem = Minus Suite\n",
    "#\n",
    "# NOTE:\n",
    "#   The use of explicit \"Indent\", \"Dedent\", \"Newline\" follows what python does\n",
    "#   (cf. https://github.com/python/cpython/blob/master/Grammar/Grammar)\n",
    "#\n",
    "# TODO:\n",
    "#   - support line/column infomation\n",
    "#\n",
    "\n",
    "import dataclasses\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Token:\n",
    "    name : None\n",
    "    info : None  = None\n",
    "    line_num : None = None\n",
    "    column_num : None = None\n",
    "\n",
    "\n",
    "def handle_quoted_string(s, m):\n",
    "    result, num = read_string_literal(s)\n",
    "    return s[num:], Token(name='String', info=result)\n",
    "\n",
    "\n",
    "yaml_token_rule= [\n",
    "    # Regex, Action\n",
    "    #   where Action: (str, match) -> (str, Optional[Token])\n",
    "    ['#',                              lambda s, m: ('',    None)],\n",
    "    ['-(?!\\S)' ,                       lambda s, m: (s[1:], Token('Minus'))],\n",
    "    ['\\\"',                             handle_quoted_string],\n",
    "    ['([_a-zA-Z][a-zA-Z0-9]*):(?!\\S)', lambda s, m: (s[m.end():], Token('Key', m.group(1)))],\n",
    "    ['\\S*',                            lambda s, m: (s[m.end():], Token('String', s[:m.end()]))],\n",
    "]\n",
    "\n",
    "\n",
    "def run_rule(s, rule): # str, List[MatchAction] -> str, Optional[Token]\n",
    "    import re    \n",
    "    for regex, action in rule:\n",
    "        m = re.match(regex, s)\n",
    "        if not m:\n",
    "            continue\n",
    "        s, token = action(s, m)\n",
    "        return s, token\n",
    "    assert False, f\"No matching token found: \\\"{s}\\\"\"\n",
    "\n",
    "\n",
    "def tokenize_content(text, rule):\n",
    "    column_num = 0\n",
    "    tokens = []\n",
    "    while len(text) > 0:\n",
    "        old_text = text\n",
    "        text, token = run_rule(text, rule)\n",
    "        if token:\n",
    "            token.column_num = column_num\n",
    "            tokens += [token]\n",
    "        text = text.lstrip()\n",
    "        column_num += len(old_text) - len(text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize(text, rule, yaml_hack=True):\n",
    "    import io\n",
    "    inp = io.StringIO(text)\n",
    "    indent_stack = [0]\n",
    "    line_num = 0\n",
    "    physical_line_num = 0\n",
    "    while True:\n",
    "        line = inp.readline()\n",
    "        physical_line_num += 1\n",
    "        line_num = physical_line_num\n",
    "        if line == '':\n",
    "            break\n",
    "            \n",
    "        # continue read \"logical\" line (backslash)\n",
    "        while line[-1:] == '\\\\':\n",
    "            line += inp.readline()\n",
    "            physical_line_num += 1\n",
    "\n",
    "        # ignore empty line\n",
    "        line_lstrip = line.lstrip()\n",
    "        if line_lstrip == '':\n",
    "            continue\n",
    "\n",
    "        content_tokens = tokenize_content(line_lstrip, rule)\n",
    "        \n",
    "        # ignore empty line due to comment\n",
    "        if len(content_tokens) == 0:\n",
    "            continue\n",
    "\n",
    "        indent = len(line) - len(line_lstrip)\n",
    "        \n",
    "        # push indent stack\n",
    "        if indent_stack[-1] < indent:\n",
    "            indent_stack += [indent]\n",
    "            yield Token('Indent')\n",
    "\n",
    "        # pop indent stack\n",
    "        if indent < indent_stack[-1]:\n",
    "            while indent < indent_stack[-1]:\n",
    "                indent_stack.pop()\n",
    "                yield Token('Dedent')\n",
    "            assert indent == indent_stack[-1], f\"No matching indentation on dedent: \\\"{line_lstrip}\\\"\"\n",
    "\n",
    "        # fille line/column number\n",
    "        for token in content_tokens:\n",
    "            token.line_num = line_num\n",
    "            token.column_num += indent\n",
    "\n",
    "        if yaml_hack:\n",
    "            # fake line and indent when \"- \" in order to support e.g. \"- x: y\"\n",
    "            if content_tokens[0].name == 'Minus':\n",
    "                indent_stack += [indent + 2]\n",
    "                yield content_tokens[0]\n",
    "                for name in ['Newline', 'Indent']:\n",
    "                    yield Token(name, line_num=line_num)\n",
    "                content_tokens = content_tokens[1:]\n",
    "                if len(content_tokens) == 0:\n",
    "                    continue\n",
    "\n",
    "        yield from iter(content_tokens)\n",
    "        yield Token('Newline', line_num=line_num)\n",
    "\n",
    "    # flush indent stack\n",
    "    while 0 < indent_stack[-1]:\n",
    "        indent_stack.pop()\n",
    "        yield Token('Dedent', line_num=line_num)\n",
    "\n",
    "    yield Token('EOI', line_num=line_num)\n",
    "\n",
    "ex03 = \"\"\"\n",
    "a:\n",
    "  b: c\n",
    "\"\"\"\n",
    "\n",
    "ex04 = \"\"\"\n",
    "- a\n",
    "- b: c\n",
    "\"\"\"\n",
    "\n",
    "list(tokenize(ex03, yaml_token_rule, yaml_hack=True))\n",
    "list(tokenize(ex04, yaml_token_rule, yaml_hack=True))\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class LookaheadIterator:\n",
    "    def __init__(self, orig_iter, eoi):\n",
    "        self.buffer = []\n",
    "        self.orig_iter = orig_iter\n",
    "        self.eoi = eoi\n",
    "        \n",
    "    def next_or_eoi(self):\n",
    "        try:\n",
    "            return next(self.orig_iter)\n",
    "        except StopIteration:\n",
    "            return self.eoi\n",
    "    \n",
    "    def lookahead(self, n):\n",
    "        while len(self.buffer) < n:\n",
    "            self.buffer += [self.next_or_eoi()]\n",
    "        return self.buffer[:n]\n",
    "    \n",
    "    def __next__(self):\n",
    "        if len(self.buffer) > 0:\n",
    "            elem, *self.buffer = self.buffer\n",
    "            return elem\n",
    "        return self.next_or_eoi()\n",
    "\n",
    "\n",
    "class Parser:\n",
    "    def __init__(self, token_gen, debug=False):\n",
    "        self.gen = LookaheadIterator(token_gen, Token('EOI'))\n",
    "        self.debug = debug\n",
    "\n",
    "    def run(self):\n",
    "        pass\n",
    "\n",
    "    # consume\n",
    "    def c(self, name):\n",
    "        tok, = self.gen.lookahead(1)\n",
    "        assert tok.name == name, f\"Expected <{name}>, but found <{tok}>\"\n",
    "        next(self.gen)\n",
    "        return tok.info\n",
    "\n",
    "    # match\n",
    "    def m(self, name):\n",
    "        tok, = self.gen.lookahead(1)        \n",
    "        return tok.name == name\n",
    "    \n",
    "    def p_input(self):\n",
    "        while self.m('Newline'): self.c('Newline')\n",
    "        expr = self.p_expr()\n",
    "        if self.debug:\n",
    "            print(f\"[debug:p_input] expr = {expr}\")\n",
    "        while self.m('Newline'): self.c('Newline')\n",
    "        self.c('EOI')\n",
    "        return expr\n",
    "\n",
    "    def p_expr(self):\n",
    "        if self.m('String'):\n",
    "            return self.p_simple_expr()\n",
    "        if self.m('Key'):\n",
    "            return self.p_dict()\n",
    "        if self.m('Minus'):\n",
    "            return self.p_list()\n",
    "        assert False, f\"Expected [Expr], but found <{self.gen.lookahead(1)[0]}>\"\n",
    "\n",
    "    def p_simple_expr(self):\n",
    "        value = self.c('String')\n",
    "        self.c('Newline')\n",
    "        return value\n",
    "\n",
    "    def p_suite(self):\n",
    "        if self.m('String'):\n",
    "            return self.p_simple_expr()            \n",
    "        if self.m('Newline'):\n",
    "            self.c('Newline')\n",
    "            self.c('Indent')\n",
    "            expr = self.p_expr()\n",
    "            self.c('Dedent')\n",
    "            return expr\n",
    "        assert False, f\"Expected [Suite], but found <{self.gen.lookahead(1)[0]}>\"\n",
    "\n",
    "    def p_dict(self):\n",
    "        items = []\n",
    "        items += [self.p_dict_item()]\n",
    "        while self.m('Key'):\n",
    "            items += [self.p_dict_item()]\n",
    "        return dict(items)\n",
    "    \n",
    "    def p_dict_item(self):\n",
    "        key = self.c('Key')\n",
    "        value = self.p_suite()\n",
    "        return (key, value)\n",
    "\n",
    "    def p_list(self):\n",
    "        ls = []\n",
    "        ls += [self.p_list_item()]\n",
    "        while self.m('Minus'):\n",
    "            ls += [self.p_list_item()]\n",
    "        return ls  \n",
    "\n",
    "    def p_list_item(self):\n",
    "        self.c('Minus')\n",
    "        return self.p_suite()\n",
    "\n",
    "\n",
    "def parse(text, debug=False, yaml_hack_tokenizer=True):\n",
    "    if debug:\n",
    "        token_gen = tokenize(text, yaml_token_rule, yaml_hack_tokenizer)\n",
    "        print(f\"[debug:parse] tokens = \", *list(token_gen), sep='\\n')\n",
    "    token_gen = tokenize(text, yaml_token_rule, yaml_hack_tokenizer)\n",
    "    parser = Parser(token_gen, debug=debug)\n",
    "    return parser.p_input()\n",
    "\n",
    "\n",
    "#\n",
    "# Small tests\n",
    "#\n",
    "\n",
    "ex00 = \"\"\"\n",
    "k1: v1\n",
    "k2: v2\n",
    "\"\"\"\n",
    "\n",
    "ex01 = \"\"\"\n",
    "- a\n",
    "- b\n",
    "\"\"\"\n",
    "\n",
    "ex02 = \"\"\"\n",
    "a:\n",
    "  - b\n",
    "\"\"\"\n",
    "\n",
    "ex03 = \"\"\"\n",
    "a:\n",
    "  b: c\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ex04 = \"\"\"\n",
    "- a\n",
    "\"\"\"\n",
    "\n",
    "ex05 = \"\"\"\n",
    "a:\n",
    "  b\n",
    "\"\"\"\n",
    "\n",
    "ex06 = \"\"\"\n",
    "-\n",
    "  - v1\n",
    "\"\"\"\n",
    "\n",
    "ex07 = \"\"\"\n",
    "a:\n",
    "  b: c\n",
    "d: e\n",
    "\"\"\"\n",
    "\n",
    "ex08 = \"\"\"\n",
    "a-b:c/d\\e\n",
    "\"\"\"\n",
    "\n",
    "ex09 = \"\"\"\n",
    "\"abc \\\\n \\'x - : # y\\' \"\n",
    "\"\"\"\n",
    "\n",
    "ex10 = \"\"\"\n",
    "a: b\n",
    "  # xyz\n",
    "c: d\n",
    "\"\"\"\n",
    "\n",
    "ex11 = \"\"\"\\\n",
    "- \n",
    "  - a\n",
    "  c: d\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "yaml_hack_ex00 = \"\"\"\n",
    "- a: b\n",
    "  c: d\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "parse(ex00, debug=False)\n",
    "parse(ex01, debug=False)\n",
    "parse(ex02, debug=False)\n",
    "parse(ex03, debug=False)\n",
    "parse(ex04, debug=False)\n",
    "parse(ex05, debug=False)\n",
    "parse(ex06, debug=False)\n",
    "parse(ex07, debug=False)\n",
    "parse(ex08, debug=False)\n",
    "parse(ex09, debug=False)\n",
    "parse(yaml_hack_ex00, debug=False)\n",
    "\n",
    "def check_throw(func, error_str):\n",
    "    try:\n",
    "        func()\n",
    "    except:\n",
    "        import sys\n",
    "        error_type, error, trace = sys.exc_info()\n",
    "        assert error_str == str(error)\n",
    "    else:\n",
    "        assert False, \"check_throw failed\"\n",
    "\n",
    "check_throw(\n",
    "    lambda: parse(yaml_hack_ex00, debug=False, yaml_hack_tokenizer=False),\n",
    "    \"Expected [Suite], but found <Token(name='Key', info='a', line_num=2, column_num=2)>\")\n",
    "\n",
    "check_throw(\n",
    "    lambda: parse(ex11),\n",
    "    \"Expected <Dedent>, but found <Token(name='Key', info='c', line_num=3, column_num=2)>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
